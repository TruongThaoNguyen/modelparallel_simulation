[rank 0] -> n0
[rank 1] -> n1
[rank 2] -> n2
[rank 3] -> n3
[rank 4] -> n4
[rank 5] -> n5
[rank 6] -> n6
[rank 7] -> n7
[rank 8] -> n8
[rank 9] -> n9
[rank 10] -> n10
[rank 11] -> n11
[rank 12] -> n12
[rank 13] -> n13
[rank 14] -> n14
[rank 15] -> n15
[rank 16] -> n16
[rank 17] -> n17
[rank 18] -> n18
[rank 19] -> n19
[rank 20] -> n20
[rank 21] -> n21
[rank 22] -> n22
[rank 23] -> n23
[rank 24] -> n24
[rank 25] -> n25
[rank 26] -> n26
[rank 27] -> n27
[rank 28] -> n28
[rank 29] -> n29
[rank 30] -> n30
[rank 31] -> n31
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/privatization' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'exception/cutpath' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/display-timing' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/allreduce' to 'lr'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'contexts/nthreads' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'cpu/optim' to 'Lazy'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'maxmin/precision' to '1e-4'
[0.000000] [surf_parse/INFO] The custom configuration 'network/model' is already defined by user!
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/optim' to 'Lazy'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/cpu-threshold' to '0.00000000001'
[0.000000] [surf_parse/INFO] The custom configuration 'smpi/display-timing' is already defined by user!
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/host-speed' to '50000000000.0'
[0.000000] [smpi_coll/INFO] Switch to algorithm lr for collective allreduce
[3.207425] [smpi_kernel/INFO] Simulated time: 3.20742 seconds. 

The simulation took 1765.48 seconds (after parsing and platform setup)
44.3475 seconds were actual computation of the application
Layer 0 [3211264.000000,1728.000000,86704128.0]
Layer 1 [3211264.000000,36864.000000,1849688064.0]
Layer 2 [802816.000000,0.000000,205520896.0]
Layer 3 [1605632.000000,73728.000000,924844032.0]
Layer 4 [1605632.000000,147456.000000,1849688064.0]
Layer 5 [401408.000000,0.000000,205520896.0]
Layer 6 [802816.000000,294912.000000,924844032.0]
Layer 7 [802816.000000,589824.000000,1849688064.0]
Layer 8 [802816.000000,589824.000000,1849688064.0]
Layer 9 [200704.000000,0.000000,205520896.0]
Layer 10 [401408.000000,1179648.000000,924844032.0]
Layer 11 [401408.000000,2359296.000000,1849688064.0]
Layer 12 [401408.000000,2359296.000000,1849688064.0]
Layer 13 [100352.000000,0.000000,205520896.0]
Layer 14 [100352.000000,2359296.000000,462422016.0]
Layer 15 [100352.000000,2359296.000000,462422016.0]
Layer 16 [100352.000000,2359296.000000,462422016.0]
Layer 17 [25088.000000,0.000000,51380224.0]
Layer 18 [4096.000000,102760448.000000,102760448.0]
Layer 19 [4096.000000,16777216.000000,16777216.0]
Layer 20 [1000.000000,4096000.000000,4096000.0]
Total weight 553376512.000000 bytes
Start training	0.000002
Start FW	0.000002
End FW	0.014069
Start BW	0.014069
End BW	0.028158
Start Weight update	0.028158
End Weight update	0.399874
Start FW	0.399876
End FW	0.413943
Start BW	0.413943
End BW	0.429100
Start Weight update	0.429100
End Weight update	0.800817
Start FW	0.800817
End FW	0.815161
Start BW	0.815161
End BW	0.830240
Start Weight update	0.830240
End Weight update	1.201920
Start FW	1.201920
End FW	1.216283
Start BW	1.216283
End BW	1.231380
Start Weight update	1.231380
End Weight update	1.603010
Start FW	1.603010
End FW	1.617220
Start BW	1.617220
End BW	1.632246
Start Weight update	1.632246
End Weight update	2.003962
Start FW	2.003962
End FW	2.018109
Start BW	2.018109
End BW	2.033136
Start Weight update	2.033136
End Weight update	2.404852
Start FW	2.404852
End FW	2.419151
Start BW	2.419151
End BW	2.434179
Start Weight update	2.434179
End Weight update	2.805876
Start FW	2.805876
End FW	2.819944
Start BW	2.819944
End BW	2.834971
Start Weight update	2.834971
End Weight update	3.206687
End training	3.206842
