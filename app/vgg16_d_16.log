[rank 0] -> n0
[rank 1] -> n1
[rank 2] -> n2
[rank 3] -> n3
[rank 4] -> n4
[rank 5] -> n5
[rank 6] -> n6
[rank 7] -> n7
[rank 8] -> n8
[rank 9] -> n9
[rank 10] -> n10
[rank 11] -> n11
[rank 12] -> n12
[rank 13] -> n13
[rank 14] -> n14
[rank 15] -> n15
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/privatization' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'exception/cutpath' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/display-timing' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/allreduce' to 'lr'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'contexts/nthreads' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'cpu/optim' to 'Lazy'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'maxmin/precision' to '1e-4'
[0.000000] [surf_parse/INFO] The custom configuration 'network/model' is already defined by user!
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/optim' to 'Lazy'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/cpu-threshold' to '0.00000000001'
[0.000000] [surf_parse/INFO] The custom configuration 'smpi/display-timing' is already defined by user!
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/host-speed' to '50000000000.0'
[0.000000] [smpi_coll/INFO] Switch to algorithm lr for collective allreduce
[3.338758] [smpi_kernel/INFO] Simulated time: 3.33876 seconds. 

The simulation took 1665.77 seconds (after parsing and platform setup)
25.6625 seconds were actual computation of the application
Layer 0 [3211264.000000,1728.000000,86704128.0]
Layer 1 [3211264.000000,36864.000000,1849688064.0]
Layer 2 [802816.000000,0.000000,205520896.0]
Layer 3 [1605632.000000,73728.000000,924844032.0]
Layer 4 [1605632.000000,147456.000000,1849688064.0]
Layer 5 [401408.000000,0.000000,205520896.0]
Layer 6 [802816.000000,294912.000000,924844032.0]
Layer 7 [802816.000000,589824.000000,1849688064.0]
Layer 8 [802816.000000,589824.000000,1849688064.0]
Layer 9 [200704.000000,0.000000,205520896.0]
Layer 10 [401408.000000,1179648.000000,924844032.0]
Layer 11 [401408.000000,2359296.000000,1849688064.0]
Layer 12 [401408.000000,2359296.000000,1849688064.0]
Layer 13 [100352.000000,0.000000,205520896.0]
Layer 14 [100352.000000,2359296.000000,462422016.0]
Layer 15 [100352.000000,2359296.000000,462422016.0]
Layer 16 [100352.000000,2359296.000000,462422016.0]
Layer 17 [25088.000000,0.000000,51380224.0]
Layer 18 [4096.000000,102760448.000000,102760448.0]
Layer 19 [4096.000000,16777216.000000,16777216.0]
Layer 20 [1000.000000,4096000.000000,4096000.0]
Total weight 553376512.000000 bytes
Start training	0.000001
Start FW	0.000002
End FW	0.028124
Start BW	0.028124
End BW	0.056265
Start Weight update	0.056265
End Weight update	0.415832
Start FW	0.415832
End FW	0.443955
Start BW	0.443955
End BW	0.473113
Start Weight update	0.473113
End Weight update	0.832792
Start FW	0.832801
End FW	0.861161
Start BW	0.861161
End BW	0.895053
Start Weight update	0.895053
End Weight update	1.254606
Start FW	1.254607
End FW	1.282729
Start BW	1.282729
End BW	1.311899
Start Weight update	1.311899
End Weight update	1.671400
Start FW	1.671400
End FW	1.699735
Start BW	1.699735
End BW	1.728641
Start Weight update	1.728641
End Weight update	2.088159
Start FW	2.088159
End FW	2.116494
Start BW	2.116494
End BW	2.145398
Start Weight update	2.145398
End Weight update	2.504909
Start FW	2.504909
End FW	2.533032
Start BW	2.533032
End BW	2.561939
Start Weight update	2.561939
End Weight update	2.921474
Start FW	2.921475
End FW	2.949809
Start BW	2.949809
End BW	2.978713
Start Weight update	2.978713
End Weight update	3.338277
End training	3.338277
