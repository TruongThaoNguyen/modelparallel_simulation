[rank 0] -> n0
[rank 1] -> n1
[rank 2] -> n2
[rank 3] -> n3
[rank 4] -> n4
[rank 5] -> n5
[rank 6] -> n6
[rank 7] -> n7
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/privatization' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'exception/cutpath' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/display-timing' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/allreduce' to 'lr'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'contexts/nthreads' to '1'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'cpu/optim' to 'Lazy'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'maxmin/precision' to '1e-4'
[0.000000] [surf_parse/INFO] The custom configuration 'network/model' is already defined by user!
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/optim' to 'Lazy'
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/cpu-threshold' to '0.00000000001'
[0.000000] [surf_parse/INFO] The custom configuration 'smpi/display-timing' is already defined by user!
[0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/host-speed' to '50000000000.0'
[0.000000] [smpi_coll/INFO] Switch to algorithm lr for collective allreduce
[3.592671] [smpi_kernel/INFO] Simulated time: 3.59267 seconds. 

The simulation took 73.2264 seconds (after parsing and platform setup)
11.8023 seconds were actual computation of the application
Layer 0 [3211264.000000,1728.000000,86704128.0]
Layer 1 [3211264.000000,36864.000000,1849688064.0]
Layer 2 [802816.000000,0.000000,205520896.0]
Layer 3 [1605632.000000,73728.000000,924844032.0]
Layer 4 [1605632.000000,147456.000000,1849688064.0]
Layer 5 [401408.000000,0.000000,205520896.0]
Layer 6 [802816.000000,294912.000000,924844032.0]
Layer 7 [802816.000000,589824.000000,1849688064.0]
Layer 8 [802816.000000,589824.000000,1849688064.0]
Layer 9 [200704.000000,0.000000,205520896.0]
Layer 10 [401408.000000,1179648.000000,924844032.0]
Layer 11 [401408.000000,2359296.000000,1849688064.0]
Layer 12 [401408.000000,2359296.000000,1849688064.0]
Layer 13 [100352.000000,0.000000,205520896.0]
Layer 14 [100352.000000,2359296.000000,462422016.0]
Layer 15 [100352.000000,2359296.000000,462422016.0]
Layer 16 [100352.000000,2359296.000000,462422016.0]
Layer 17 [25088.000000,0.000000,51380224.0]
Layer 18 [4096.000000,102760448.000000,102760448.0]
Layer 19 [4096.000000,16777216.000000,16777216.0]
Layer 20 [1000.000000,4096000.000000,4096000.0]
Total weight 553376512.000000 bytes
Start training	0.000003
Start FW	0.000003
End FW	0.056243
Start BW	0.056243
End BW	0.112500
Start Weight update	0.112500
End Weight update	0.448022
Start FW	0.448022
End FW	0.504261
Start BW	0.504261
End BW	0.561768
Start Weight update	0.561768
End Weight update	0.896912
Start FW	0.896912
End FW	0.953505
Start BW	0.953505
End BW	1.011376
Start Weight update	1.011376
End Weight update	1.346786
Start FW	1.346786
End FW	1.403025
Start BW	1.403025
End BW	1.460330
Start Weight update	1.460330
End Weight update	1.795604
Start FW	1.795617
End FW	1.852088
Start BW	1.852088
End BW	1.909744
Start Weight update	1.909744
End Weight update	2.245139
Start FW	2.245140
End FW	2.301378
Start BW	2.301378
End BW	2.358711
Start Weight update	2.358711
End Weight update	2.693993
Start FW	2.693993
End FW	2.750461
Start BW	2.750461
End BW	2.807790
Start Weight update	2.807790
End Weight update	3.143187
Start FW	3.143187
End FW	3.199426
Start BW	3.199426
End BW	3.256773
Start Weight update	3.256773
End Weight update	3.592055
End training	3.592228
